---
title: "Analysis of Human Breast Cancer Cells"
author: "Kelsey Dang"
date: "10/29/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Exploratory Data Analysis

Use the read.csv() function to read the CSV (comma-separated values) file containing the data.
```{r}
wisc.df <- read.csv("WisconsinCancer.csv")
head(wisc.df)
```

Now, see how many patients we have.
```{r}
nrow(wisc.df)
```

How many patients are malignant?
```{r}
table(wisc.df$diagnosis)
```

Creating wisc.data
```{r}
# Convert the features of the data: wisc.data
wisc.data <- as.matrix(wisc.df[,3:32])

# Set the row names of wisc.data
row.names(wisc.data) <- wisc.df$id
head(wisc.data)
```

Store the diagnosis for reference in the future as a separate vector

Setup a separate new vector called **diagnosis** that contains the data from the diagnosis column of the original dataset. We will use this later to check our results.  

```{r}
# Create diagnosis vector for later 
diagnosis <- wisc.df$diagnosis
```
## Exploratory Data Analysis

Q1. How many observations are in this dataset?
```{r}
nrow(wisc.df)
```

Q2. How many of the observations have a malignant diagnosis?
```{r}
table(wisc.df$diagnosis)
```

Q3. How many variables/features in the data are suffixed with _mean?
```{r}
grep("_mean", colnames(wisc.df), value = TRUE)
```

Use *length* to see the length
```{r}
length(grep("_mean", colnames(wisc.df), value = TRUE))
```

# 2. Principal Component Analysis

## Performing PCA
The next step in your analysis is to perform principal component analysis (PCA) on wisc.data.

It is important to check if the data need to be scaled before performing PCA. Recall two common reasons for scaling data include:  
  * The input variables use different units of measurement  
  * The input variables have significantly different variances  

Check the mean and standard deviation of the features (i.e. columns) of the wisc.data to determine if the data should be scaled. Use the colMeans() and apply() functions like you’ve done before.
```{r}
# Check column means and standard deviations
colMeans(wisc.data)

apply(wisc.data,2,sd)
```

The function <span style="color:red">round()</span> rounds the values in its first arg to the specified num of decimal places (default 0) or the second arg.
```{r}
round(apply(wisc.data, 2, sd), 3)
```

These values look very different so I will use **scale=TRUE** when I run PCA
```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp( wisc.data, scale = TRUE )
summary(wisc.pr)
```

PC1 v. PC2 plot

Color by cancer/non-cancer
```{r}
# Scatter plot observations by components 1 and 2
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=diagnosis, xlab = "PC1", ylab = "PC2")
```

You can see the split between cancerous and non-cancerous!

Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?  
```{r}
x <- summary(wisc.pr)
x$importance[,1]
# By using importance can print out a specific col/row
# 0.4427
```

Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
```{r}
which(x$importance[3,] > 0.7)[1]
```

Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
```{r}
which(x$importance[3,] > 0.9)[1]
```

## Communicating PCA Results
In this section we will check your understanding of the PCA results, in particular the loadings and variance explained. The loadings, represented as vectors, explain the mapping from the original features to the principal components. The principal components are naturally ordered from the most variance explained to the least variance explained.

Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

```{r}
wisc.pr$rotation["concave.points_mean",1]
```


Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

```{r}
x <- summary(wisc.pr)
which(x$importance["Cumulative Proportion",] > 0.8)[1]
```


# 3. Hierarchical Clustering
## Hierarchical clustering of case data

The goal of this section is to do hierarchical clustering of the observations. Recall from our last class that this type of clustering does not assume in advance the number of natural groups that exist in the data.

As part of the preparation for hierarchical clustering, the distance between all pairs of observations are computed. Furthermore, there are different ways to link clusters together, with single, complete, and average being the most common linkage methods.

Scale the wisc.data data and assign the result to data.scaled.

```{r}
# Scale the wisc.data data: data.scaled
data.scaled <- scale(wisc.data)

```

Calculate the Euclidean distances between all pairs of observations.
```{r}
data.dist <- dist(data.scaled)
```

Create a hierarchical clustering model using complete linkage
```{r}
wisc.hclust <- hclust(data.dist, method = "complete")
```

## Results of Hierarchical Clustering

Let’s use the hierarchical clustering model you just created to determine a height (or distance between clusters) where a certain number of clusters exists.

Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust)
abline(wisc.hclust, col="red", lty=2)
```
### Selecting number of clusters
In this section, you will compare the outputs from your hierarchical clustering model to the actual diagnoses.  
<span style="color:green">Unsupervised learning</span> - normally when performing unsupervised learning, a target variable (i.e. known answer or labels) aren't available  
<span style="color:green">Supervised learning</span> - when performing supervised learning, you're trying to predict some target variable of interest and that target variable is available in the original data



Use cutree() to cut the tree so that it has 4 clusters. Assign the output to the variable wisc.hclust.clusters.
```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
table(wisc.hclust.clusters, diagnosis)
```

# 4. K-means Clustering

In this section, you will create a k-means clustering model on the Wisconsin breast cancer data and compare the results to the actual diagnoses and the results of your hierarchical clustering model. Take some time to see how each clustering model performs in terms of separating the two diagnoses and how the clustering models compare to each other.  
  
Create a k-means model on wisc.data, assigning the result to wisc.km. Be sure to create 2 clusters, corresponding to the actual number of diagnosis. Also, remember to scale the data (with the scale() function and repeat the algorithm 20 times (by setting setting the value of the nstart argument appropriately). Running multiple times such as this will help to find a well performing model.  

```{r}
wisc.km <- kmeans(scale(wisc.data), centers = 2, nstart = 20)
#head(wisc.km)
```

Use the table() function to compare the cluster membership of the k-means model (wisc.km$cluster) to the actual diagnoses contained in the diagnosis vector.
```{r}
table(wisc.km$cluster, diagnosis)
```


Use the table() function to compare the cluster membership of the k-means model (wisc.km$cluster) to your hierarchical clustering model from above (wisc.hclust.clusters). Recall the cluster membership of the hierarchical clustering model is contained in wisc.hclust.clusters object.
```{r}
table(wisc.hclust.clusters, wisc.km$cluster)
```

# 5. Combining Methods
  * Recall that the PCA model requires significantly fewer features to describe 70%, 80%, 95% of the variability of the data.  
  * In addition to normalizing data and potentially avoiding over-fitting, PCA also uncorrelates the variables, sometimes improving the performance of other modeling techniques  

Let's see if PCA improves or degrades the performance of heirarchical clustering.

Using the minimum num of principle components required to describe at LEAST 90% of the variability in the data, create a hierarchical clustering model with the linkage <span style="color:blue">method = "ward.D2"</span>. We use Ward's criterion here because it is based on multidimensional variance like principal components analysis.
```{r}
wisc.pr.hclust <- hclust( dist(wisc.pr$x[, 1:7]) , method = "ward.D2")
```

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
table(grps, diagnosis)
```

```{r}
plot(wisc.pr$x[,1:2], col=grps)
```

```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```

**Note:** the color swap here as the hclust cluster1 is mostly "M" and cluster2 is mostly "B" as we saw from the results of calling table(). To match things up we can turn our groups into a factor and reorder the levels so cluster2 comes first.  
```{r}
g <- as.factor(grps)
levels(g)
```

```{r}
g <- relevel(g,2)
levels(g)
```

```{r}
# Plot using our re-ordered factor 
plot(wisc.pr$x[,1:2], col=g)
```

# 6. Sensitivity/Specificity
  * <span style="color:purple">Sensitivity:</span>
    - refers to a test's ability to correctly detect ill patients who do have the condition.  
    - In our example, sensitivity is the total number of samples in the cluster identified as predominately malignant (cancerous) divided by the total number of known malignant samples.  
  * <span style="color:purple">Specificity:</span>
    - relates to a test's ability to correctly reject healthy patients without a condition.  
    - In our example, specificity is the proportion of benign (not cancerous) samples in the cluster identified as predominantly benign that are known to be benign.  

# 7. Prediction
We will use the **predict()** function that will take our PCA model from before and new cancer cell data and project that data onto our PCA space.

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=diagnosis)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

Q17. Which of these new patients should we prioritize for follow up based on your results?
Patient 2






